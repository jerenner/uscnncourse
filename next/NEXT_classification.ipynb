{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal vs. background classification in NEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import matplotlib        as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy             as np\n",
    "import tables            as tb\n",
    "\n",
    "from glob    import glob\n",
    "from IPython import display\n",
    "\n",
    "import torch\n",
    "import torch.nn            as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim         as optim\n",
    "\n",
    "from torch.autograd   import Variable\n",
    "from torch.autograd   import Function\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "mpl.rcParams.update({'font.size': 14})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enable/disable CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_available = torch.cuda.is_available()\n",
    "print(\"CUDA Available:\", CUDA_available)\n",
    "\n",
    "enable_CUDA  = True                                # enable CUDA by default if it is available\n",
    "CUDA_enabled = (enable_CUDA and CUDA_available)\n",
    "if (CUDA_enabled):\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "    \n",
    "print(\"CUDA Enabled  :\", CUDA_enabled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O background.h5 https://www.dropbox.com/s/twrlgugolrmuqyr/background.h5?dl=0\n",
    "!wget -O signal.h5 https://www.dropbox.com/s/10i52opac7j7fpx/signal.h5?dl=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of key variables\n",
    "The directory structure followed is:\n",
    "\n",
    "`$NEXTHOME/data`   -- contains datafiles `signal.h5` and `background.h5`<br>\n",
    "`$NEXTHOME/models` -- contains saved models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data dimensions\n",
    "xdim = 20\n",
    "ydim = 20\n",
    "zdim = 60\n",
    "\n",
    "# directory structure\n",
    "modelsdir = \"models\"\n",
    "if(not os.path.isdir(modelsdir)):\n",
    "    os.mkdir(modelsdir)\n",
    "\n",
    "datafile_signal     = \"signal.h5\"\n",
    "datafile_background = \"background.h5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview events\n",
    "The events are 20x20x60 maps of simulated voxelized tracks. One can get a good idea of what these tracks look like by summing over the z-dimension and plotting this projection in x-y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evtnum = 0\n",
    "\n",
    "with tb.open_file(datafile_background, \"r\") as file:\n",
    "    event = file.root.maps[evtnum]\n",
    "\n",
    "plt.imshow(np.sum(event,axis=2),cmap='jet')\n",
    "plt.imshow(np.sum(event,axis=2),cmap='jet')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "Here a Pytorch `Dataset` is created for batching 3D maps of NEXT track topologies. The files containing signal and background events are provided, followed by a range of events. The idea is that the training, validation, and test sets can be created using events from the same files via differing ranges of events. The flag `load_to_memory`, when set to `True`, loads all events into memory at once for faster batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NEXTDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, datafile_signal, datafile_background, nstart=0, nend=0, load_to_memory=True):\n",
    "        \n",
    "        # Save inputs for later use.\n",
    "        self.datafile_signal     = datafile_signal\n",
    "        self.datafile_background = datafile_background\n",
    "        self.load_to_memory      = load_to_memory\n",
    "        \n",
    "        # Read datafiles and get the tracks\n",
    "        signal     = tb.open_file(datafile_signal    , \"r\").root.maps \n",
    "        background = tb.open_file(datafile_background, \"r\").root.maps\n",
    "        \n",
    "        # Get the number of signal and background events and ensure that we have the same.\n",
    "        nsignal     = signal    .shape[0]\n",
    "        nbackground = background.shape[0]\n",
    "        nmin        = min(nsignal, nbackground)\n",
    "        \n",
    "        # Ensure nend > nstart.\n",
    "        if (nend <= nstart):\n",
    "            nend = nmin\n",
    "            \n",
    "        # Load everything at once.\n",
    "        if (load_to_memory):\n",
    "            self.data   = np.vstack((signal[nstart:nend]      , background[nstart:nend]))\n",
    "            self.labels = np.vstack((np.ones((nend-nstart, 1)), np.zeros((nend-nstart, 1))))\n",
    "                                    \n",
    "        # Save the final values of nstart and nend.\n",
    "        self.nstart = nstart\n",
    "        self.nend   = nend\n",
    "        \n",
    "        print(\"Created dataset from events\", nstart, \"to\", nend, \"of\", nmin, \"available signal and background events\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return 2*(self.nend-self.nstart)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Choose the data, label pair at the specified index if everything is already loaded into memory.\n",
    "        if (self.load_to_memory):\n",
    "            event = torch.tensor(self.data  [idx]).float()\n",
    "            label = torch.tensor(self.labels[idx]).float()\n",
    "        \n",
    "        # Otherwise find the event in the correct file.\n",
    "        else:\n",
    "            # Second half of indices correspond to background.\n",
    "            nrange = (self.nend-self.nstart)\n",
    "            if(idx >= nrange):\n",
    "                datafile = self.datafile_background\n",
    "                idx     -= nrange\n",
    "                label    = torch.tensor([0]).float()\n",
    "            else:\n",
    "                datafile = self.datafile_signal\n",
    "                label    = torch.tensor([1]).float()\n",
    "\n",
    "            # Open the correct file and extract the element corresponding to this index.\n",
    "            with tb.open_file(datafile, \"r\") as file:\n",
    "                event = torch.tensor(file.root.maps[idx])\n",
    "                \n",
    "        return event, label\n",
    "    \n",
    "    ## probably this can be remove because we send this objects to CUDA in train() and val()\n",
    "    \n",
    "#         if (CUDA_enabled):\n",
    "#             return event.cuda(), label.cuda()\n",
    "#         else:\n",
    "#             return event, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset = NEXTDataset(datafile_signal, datafile_background, nstart=0, nend=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "event, label = Dataset[10]\n",
    "\n",
    "plt.title(f\"Event label: {int(label.item())} \")\n",
    "plt.imshow(event.sum(dim=2), cmap='jet')\n",
    "plt.imshow(event.sum(dim=2), cmap='jet')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a neural network\n",
    "Here the neural network architecture is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FCNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(xdim*ydim*zdim, 32)\n",
    "        self.fc2 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, xdim*ydim*zdim)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = FCNet()\n",
    "if(CUDA_enabled):\n",
    "    net = net.cuda()\n",
    "summary(net, (xdim, ydim, zdim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network\n",
    "Here functions for network training and validation are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = True\n",
    "\n",
    "def train():\n",
    "    losses_epoch = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        if(CUDA_enabled):\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        \n",
    "        # optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses_epoch.append(loss.item())\n",
    "\n",
    "        if verbose and (batch_idx % 10 == 0):\n",
    "            progress = f\"Train Epoch: {epoch} [{batch_idx*batch_size:05}/{len(train_loader.dataset)}\" +\\\n",
    "                       f\" ({int(100*batch_idx/len(train_loader)):02}%)]\"\n",
    "            loss_ = f\"\\t Loss: {loss.item():.6f}\"\n",
    "            print(progress + loss_)\n",
    "            \n",
    "    return losses_epoch\n",
    "\n",
    "\n",
    "def val():\n",
    "    losses_epoch = []\n",
    "    for batch_idx, (data, target) in enumerate(val_loader):\n",
    "        \n",
    "        if(CUDA_enabled):\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "\n",
    "        # Compute the model result.\n",
    "        outputs = model(data)\n",
    "        \n",
    "        # Compute the loss.\n",
    "        loss = criterion(outputs, target)\n",
    "        \n",
    "        if verbose and (batch_idx % 10 == 0):\n",
    "#             print('Val Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\t logits_max: {:.6f}\\t logits_min: {:.6f}'.format(\n",
    "#                 epoch, batch_idx * len(data), len(val_loader.dataset),\n",
    "#                 100. * batch_idx / len(val_loader), loss.data.item(), \n",
    "#                 outputs[:,0].data.max(), outputs[:,0].data.min()))\n",
    "            pass\n",
    "\n",
    "        losses_epoch.append(loss.data.item())\n",
    "        \n",
    "#     print(\"---EPOCH\", epoch, \"AVG VAL LOSS:\", np.mean(losses_epoch))\n",
    "    return losses_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create datasets and data loaders for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "nstart_train, nend_train = 0   , 5000\n",
    "nstart_val  , nend_val   = 5000, 6000\n",
    "\n",
    "dataset_train = NEXTDataset(datafile_signal, datafile_background, nstart_train, nend_train)\n",
    "train_loader  = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "dataset_val   = NEXTDataset(datafile_signal, datafile_background, nstart_val, nend_val)\n",
    "val_loader    = DataLoader(dataset_val, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set some key flags for training:\n",
    "- **load_model**: set to True to load the specified model. If training is True, the model will be loaded before any training begins.\n",
    "- **modelfile**: the model to be loaded\n",
    "- **training**: set to True to perform training\n",
    "- **lrate**: the learning rate to use for training\n",
    "\n",
    "Note: to perform testing with an already trained model, set `load_model` to `True` and `training` to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "lrate  = 1e-2\n",
    "load_model = False\n",
    "training   = True\n",
    "modelfile  = f\"{modelsdir}/model_NEXT_0.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = np.arange(epochs)\n",
    "train_losses = np.full(len(x), np.nan)\n",
    "val_losses   = np.full(len(x), np.nan)\n",
    "\n",
    "fig = plt.figure(figsize=[15, 5])\n",
    "ax0 = fig.add_subplot(121) \n",
    "ax1 = fig.add_subplot(122)\n",
    "ax0.set_xlabel(\"EPOCHS\")\n",
    "ax1.set_xlabel(\"EPOCHS\")\n",
    "ax0.set_ylabel(\"LOSS\")\n",
    "ax1.set_ylabel(\"ACCURACY\")\n",
    "ax0.set_xlim([-0.1, epochs-1 + 0.1])\n",
    "ax1.set_xlim([-0.1, epochs-1 + 0.1])\n",
    "ax0.set_xticks(range(0, epochs))\n",
    "ax1.set_xticks(range(0, epochs))\n",
    "\n",
    "# Create your net\n",
    "model = FCNet()\n",
    "\n",
    "# Choose an Optimizer and a Loss\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lrate, betas=(0.9, 0.999), eps=1e-6, weight_decay=0)\n",
    "\n",
    "if (CUDA_enabled):\n",
    "    model.cuda()\n",
    "\n",
    "# Load the model from file.\n",
    "if (load_model):\n",
    "    model.load_state_dict(torch.load(modelfile))\n",
    "    model.eval()\n",
    "\n",
    "# Train the model.\n",
    "if (training):\n",
    "    for epoch in range(0, epochs):\n",
    "        model.train()\n",
    "        losses_epoch = train()\n",
    "        train_losses[epoch] = np.mean(losses_epoch)\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            losses_epoch = val()\n",
    "            val_losses[epoch] = np.mean(losses_epoch)\n",
    "    \n",
    "        # save model in epoch\n",
    "        torch.save(model.state_dict(), f\"{modelsdir}/model_NEXT_{epoch}.pt\")\n",
    "        \n",
    "        # update plot\n",
    "        ax0.set_title(f\"EPOCH {epoch}\")\n",
    "        ax0.scatter(x, train_losses, c=\"black\")\n",
    "        ax0.scatter(x, val_losses  , c=\"red\")\n",
    "        \n",
    "        ax1.set_title(f\"EPOCH {epoch}\")\n",
    "\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(fig)\n",
    "        \n",
    "display.clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model\n",
    "Create a test dataset and loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nstart, nend = 8000, 10000\n",
    "\n",
    "dataset_test  = NEXTDataset(datafile_signal, datafile_background, nstart, nend)\n",
    "# test_loader   = DataLoader(dataset_test, batch_size=200, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the model on each data sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal, _     = dataset_test[:nstart-nend]\n",
    "background, _ = dataset_test[nstart-nend:]\n",
    "if(CUDA_enabled):\n",
    "    signal,background = signal.cuda(), background.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = nn.Sigmoid()\n",
    "\n",
    "with torch.no_grad():\n",
    "    signal_predictions     = sigmoid(model(signal))\n",
    "    background_predictions = sigmoid(model(background))\n",
    "    \n",
    "signal_predictions     = signal_predictions    .cpu().squeeze().numpy()\n",
    "background_predictions = background_predictions.cpu().squeeze().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot model prediction distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(signal_predictions    , bins=np.linspace(0, 1, 100), histtype=\"step\", label=\"signal\")\n",
    "plt.hist(background_predictions, bins=np.linspace(0, 1, 100), histtype=\"step\", label=\"background\")\n",
    "plt.xlabel(\"PREDICTION\");\n",
    "plt.ylabel(\"Entries\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the signal efficiency vs. background rejection for several thresholds.\n",
    "- the **signal efficiency** is the number of correctly predicted (prediction >= threshold) signal events divided by the total number of signal events\n",
    "- the **background rejection** is the number of correctly predicted (prediction < threshold) background events divided by the total number of background events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npoints = 100\n",
    "signal_eff, background_rej = [],[]\n",
    "\n",
    "for th in np.arange(0, 1, 1./npoints):\n",
    "    \n",
    "    # Get the total number of each class.\n",
    "    N_signal     = len(signal_predictions)\n",
    "    N_background = len(background_predictions)\n",
    "    \n",
    "    # Get the number of correctly classified for each class.\n",
    "    correct_signal     = np.sum(signal_predictions     >= th)\n",
    "    correct_background = np.sum(background_predictions <  th)\n",
    "    \n",
    "    signal_eff    .append(1.0*correct_signal    /N_signal)\n",
    "    background_rej.append(1.0*correct_background/N_background)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(signal_eff, background_rej,color='black',label='Test', lw=2)\n",
    "plt.xlabel(\"signal efficiency\")\n",
    "plt.ylabel(\"background rejection\")\n",
    "plt.legend(loc=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things to try:\n",
    "1. Design an improved network.\n",
    "2. Evaluate individual events: plot the event along with its classification probability. Is this how you would have classified the event? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
